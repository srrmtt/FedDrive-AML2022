{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4685,"status":"ok","timestamp":1673450878796,"user":{"displayName":"Federated Learning","userId":"04552535780690637933"},"user_tz":-60},"id":"YxI0OKJ9fMt7","outputId":"ba93077e-41fc-49ae-edd8-2bee7409c408"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SX5PG63TgYhP"},"outputs":[],"source":["import torchvision\n","from sklearn.metrics import confusion_matrix  \n","import numpy as np\n","import random\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import transforms\n","from torchvision.datasets import VisionDataset\n","from torch.utils.data import Subset, DataLoader\n","from sklearn.model_selection import train_test_split\n","from PIL import Image\n","import os\n","import os.path\n","import sys\n","from torch.backends import cudnn\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from datasets import Cityscapes\n","from models import *\n","from utils import *"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n1S_Ve97ZGTp"},"outputs":[],"source":["#DEVICE = 'cuda' # 'cuda' or 'cpu'\n","DEVICE = 'cuda'\n","NUM_CLASSES = 19 # 101 + 1: There is am extra Background class that should be removed \n","\n","BATCH_SIZE = 16     # Higher batch sizes allows for larger learning rates. An empirical heuristic suggests that, when changing\n","                     # the batch size, learning rate should change by the same factor to have comparable results\n","\n","LR = 1*5e-3           # The initial Learning Rate\n","MOMENTUM = 0.9       # Hyperparameter for SGD, keep this at 0.9 when using SGD\n","WEIGHT_DECAY = 5e-5  # Regularization, you can keep this at the default -5\n","\n","NUM_EPOCHS =200      # 20/30 Total number of training epochs (iterations over dataset)\n","STEP_SIZE = 60       #20 How many epochs before decreasing learning rate (if using a step-down policy)\n","GAMMA = 0.05\n","POWER = 0.9        \n","LOG_FREQUENCY = 20"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WWS3y3_5poPS"},"outputs":[],"source":["#transformations\n","size = (256, 512) #512,1024\n","scales=(0.25, 2.)\n","cropsize=(512, 1024)\n","eval_crop=(1024, 1024)\n","eval_scales=(0.5, 0.75, 1.0, 1.25, 1.5, 1.75)\n","test_transformations = [RandomResizedCrop(eval_scales, eval_crop)]\n","train_transformations = [RandomResizedCrop(scales,cropsize),RandomHorizontalFlip(),ColorJitter(brightness=0.4,contrast=0.4,saturation=0.4)]\n","test_transformations = Compose(test_transformations)\n","train_transformations = Compose(train_transformations)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1673450880842,"user":{"displayName":"Federated Learning","userId":"04552535780690637933"},"user_tz":-60},"id":"H9f83Hhq4LAH","outputId":"0a5f205f-839c-416b-8a2b-4696c82ba405"},"outputs":[{"name":"stdout","output_type":"stream","text":["500\n","250\n","708\n","42\n","tensor([[[-0.1373, -0.2784, -0.3490,  ..., -0.8980, -0.8902, -0.8824],\n","         [-0.2471, -0.3490, -0.4118,  ..., -0.8980, -0.8980, -0.8902],\n","         [-0.4745, -0.5294, -0.5294,  ..., -0.9059, -0.9059, -0.8980],\n","         ...,\n","         [-0.2157, -0.2235, -0.2235,  ..., -0.4745, -0.4824, -0.4980],\n","         [-0.2235, -0.2314, -0.2314,  ..., -0.4745, -0.4902, -0.4824],\n","         [-0.2314, -0.2471, -0.2471,  ..., -0.4745, -0.4824, -0.4824]],\n","\n","        [[-0.0196, -0.1059, -0.2000,  ..., -0.8667, -0.8588, -0.8431],\n","         [-0.0980, -0.1686, -0.2471,  ..., -0.8745, -0.8588, -0.8510],\n","         [-0.2549, -0.3020, -0.3647,  ..., -0.8745, -0.8667, -0.8667],\n","         ...,\n","         [-0.1373, -0.1294, -0.1373,  ..., -0.4118, -0.4118, -0.4353],\n","         [-0.1373, -0.1373, -0.1373,  ..., -0.4118, -0.4275, -0.4118],\n","         [-0.1451, -0.1451, -0.1529,  ..., -0.4118, -0.4118, -0.4118]],\n","\n","        [[-0.1843, -0.2078, -0.2392,  ..., -0.8980, -0.8902, -0.8824],\n","         [-0.2863, -0.3255, -0.3569,  ..., -0.8980, -0.8980, -0.8980],\n","         [-0.4118, -0.4510, -0.5059,  ..., -0.8980, -0.8980, -0.8980],\n","         ...,\n","         [-0.2157, -0.2157, -0.2157,  ..., -0.5216, -0.5294, -0.5294],\n","         [-0.2235, -0.2157, -0.2235,  ..., -0.5059, -0.5373, -0.5137],\n","         [-0.2157, -0.2314, -0.2392,  ..., -0.5294, -0.5373, -0.5294]]])\n","<class 'torch.Tensor'>\n","torch.Size([3, 512, 1024])\n","torch.Size([512, 1024])\n"]}],"source":["train_dataset_B = Cityscapes('/content/drive/MyDrive/data/data/Cityscapes',transform=train_transformations, split='train')\n","print(len(train_dataset_B))\n","test_dataset_B = Cityscapes('/content/drive/MyDrive/data/data/Cityscapes',transform=None,split='val')\n","print(len(test_dataset_B))\n","train_dataset_A = Cityscapes('/content/drive/MyDrive/data/data/Cityscapes',transform=train_transformations, split='trainA')\n","print(len(train_dataset_A))\n","test_dataset_A = Cityscapes('/content/drive/MyDrive/data/data/Cityscapes',transform=None,split='valA')\n","print(len(test_dataset_A))\n","img,label = train_dataset_B.__getitem__(0)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9UNy0go3ptsL"},"outputs":[],"source":["train_dataloader_B = DataLoader(train_dataset_B, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, drop_last=True)\n","test_dataloader_B = DataLoader(test_dataset_B, batch_size=1, shuffle=False, num_workers=2)\n","train_dataloader_A = DataLoader(train_dataset_A, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, drop_last=True)\n","test_dataloader_A = DataLoader(test_dataset_A, batch_size=1, shuffle=False, num_workers=2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GI0zmgXzZ-e8"},"outputs":[],"source":["bisenet_model = BiSeNetV2(n_classes=NUM_CLASSES,output_aux=True,pretrained=True) #meglio toglierla perchÃ¨ genera bias\n","bisenet_model.requires_grad = True"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"85T4jE7qdS1a"},"outputs":[],"source":["criterion = nn.CrossEntropyLoss(ignore_index=255,reduction='none') # da consegna ignore_index=255\n","parameters_to_optimize = bisenet_model.parameters() \n","optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n","scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eFiWN6RO3jkn"},"outputs":[],"source":["def plotLoss(train,label):\n","  epochs = np.array([a for a in range(NUM_EPOCHS)]).reshape(NUM_EPOCHS,1)\n","  plt.figure()\n","  #plt.plot(epochs,val,label='val_loss')\n","  plt.ylim(0,max(train)+1)\n","  plt.plot(epochs,train,label=label)\n","  plt.legend()\n","  plt.show()\n","\n","def plotmIoU(train,label):\n","  epochs = np.array([a for a in range(NUM_EPOCHS)]).reshape(NUM_EPOCHS,1)\n","  plt.figure()\n","  #plt.plot(epochs,val,label='val_loss')\n","  plt.ylim(0,max(train)+0.1)\n","  plt.plot(epochs,train,label=label)\n","  plt.legend()\n","  plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nN5cVw1IfjZZ"},"outputs":[],"source":["def _fast_hist(n_classes, label_true, label_pred):\n","      mask = (label_true >= 0) & (label_true < n_classes)\n","      hist = np.bincount(\n","          n_classes * label_true[mask].astype(int) + label_pred[mask],\n","          minlength=n_classes ** 2,\n","      ).reshape(n_classes, n_classes)\n","      return hist\n","\n","\n","def compute_mIoU(y_true,y_pred):\n","  \n","  y_pred = y_pred.cpu().numpy().flatten()\n","  y_true = y_true.cpu().numpy().flatten()\n","  hist = _fast_hist(19,y_true,y_pred)\n","  gt_sum = hist.sum(axis=1)\n","  mask = (gt_sum != 0)\n","  diag = np.diag(hist)\n","  iu = diag / (gt_sum + hist.sum(axis=0) - diag)\n","  mean_iu = np.mean(iu[mask])\n","  return mean_iu"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"avy5wJKawSi_"},"outputs":[],"source":["\n","checkpoint = torch.load('/content/drive/MyDrive/magna/step2/long_esp_dataset_B_5e-3/180checkpoint.pt')\n","bisenet_model.load_state_dict(checkpoint['model_state_dict'])\n","optimizer = optim.SGD(bisenet_model.parameters(), lr=1*1e-4, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n","scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)\n","bisenet_model.to(DEVICE)\n","check_epoch = checkpoint['epoch']\n","check_loss = checkpoint['loss']\n","loss_for_epochs = checkpoint['loss_for_epochs']\n","mIoU_for_epochs = checkpoint['mIoU_for_epochs']\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dGjhngmUfK3t"},"outputs":[],"source":["net = bisenet_model.half().to(DEVICE) # this will bring the network to GPU if DEVICE is cuda\n","\n","cudnn.benchmark # Calling this optimizes runtime\n","\n","best_mIoU = 0\n","\n","for epoch in range(check_epoch,NUM_EPOCHS):\n","  print('Starting epoch {}/{}, LR = {}'.format(epoch+1, NUM_EPOCHS, scheduler.get_lr()))\n","  current_step=0\n","  # Iterate over the dataset\n","  for images, labels in train_dataloader_B:\n","    # Bring data over the device of choice\n","    \n","    images = images.half().to(DEVICE)\n","    labels = labels.half().to(DEVICE)\n","   \n","    net.train() # Sets module in training mode\n","\n","    # PyTorch, by default, accumulates gradients after each backward pass\n","    # We need to manually set the gradients to zero before starting a new iteration\n","    optimizer.zero_grad() # Zero-ing the gradients\n","\n","    # Forward pass to the network\n","    output1,output2,output3,output4,output5 = net(images)\n","    pred1 = output1.argmax(dim=1)\n","    pred2 = output2.argmax(dim=1)\n","    pred3 = output3.argmax(dim=1)\n","    pred4 = output4.argmax(dim=1)\n","    pred5 = output5.argmax(dim=1)\n","    \n","    loss1 = criterion(output1,labels.long())[labels!=255].mean()\n","    loss2 = criterion(output2,labels.long())[labels!=255].mean()\n","    loss3 = criterion(output3,labels.long())[labels!=255].mean()\n","    loss4 = criterion(output4,labels.long())[labels!=255].mean()\n","    loss5 = criterion(output5,labels.long())[labels!=255].mean()\n","\n","    loss = loss1+loss2+loss3+loss4+loss5\n","    \n","    if current_step % LOG_FREQUENCY == 0:\n","      print('Step {}, Loss {}'.format(current_step, loss.item()))\n","      \n","      mIoU = compute_mIoU(labels,pred1)\n","      print('Step {}, mIoU {}'.format(current_step, mIoU))\n","      \n","    loss.backward()\n","\n","  \n","    optimizer.step() # update weights based on accumulated gradients\n","\n","    current_step += 1\n","\n","  if epoch % 10 == 0:\n","    torch.save({\n","            'epoch': epoch,\n","            'model_state_dict': net.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'loss': loss,\n","            'loss_for_epochs' : loss_for_epochs,\n","            'mIoU_for_epochs' : mIoU_for_epochs,\n","            }, '/content/drive/MyDrive/magna/step2/long_esp_dataset_B_5e-3/'+ str(epoch) +'checkpoint.pt')\n","\n","  loss_for_epochs.append(loss.item())\n","  mIoU_for_epochs.append(mIoU)\n","  # Step the scheduler\n","  scheduler.step() \n","\n","plotLoss(loss_for_epochs)\n","plotmIoU(mIoU_for_epochs)\n","print(loss_for_epochs)\n","print(mIoU_for_epochs)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
